# HW07 – Report

> Файл: `homeworks/HW07/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Datasets

Вы выбрали 3 датасета из 4 (перечислите):

- S07-hw-dataset-01.csv
- S07-hw-dataset-02.csv  
- S07-hw-dataset-03.csv

### 1.1 Dataset A

- **Файл**: `S07-hw-dataset-01.csv`
- **Размер**: (1000 строк, 9 столбцов)
- **Признаки**: 8 числовых признаков (f01-f08) + sample_id
- **Пропуски**: отсутствуют
- **"Подлости" датасета**: признаки имеют разные масштабы и дисперсии, что требует обязательного масштабирования. Структура данных предполагает сферические кластеры.

### 1.2 Dataset B

- **Файл**: `S07-hw-dataset-02.csv`
- **Размер**: (800 строк, 4 столбцов)
- **Признаки**: 3 числовых признака (x1, x2, z_noise) + sample_id
- **Пропуски**: отсутствуют
- **"Подлости" датасета**: присутствует зашумленный признак `z_noise`, который может мешать кластеризации. Данные содержат иерархическую структуру.

### 1.3 Dataset C

- **Файл**: `S07-hw-dataset-03.csv`
- **Размер**: (1200 строк, 5 столбцов)
- **Признаки**: 4 числовых признака (x1, x2, f_corr, f_noise) + sample_id
- **Пропуски**: отсутствуют
- **"Подлости" датасета**: есть коррелированные признаки (f_corr) и зашумленный признак (f_noise). Кластеры имеют разную плотность и форму.

## 2. Protocol

Опишите ваш "честный" unsupervised-протокол.

- **Препроцессинг**: 
  - Стандартизация числовых признаков через `StandardScaler`
  - Обработка пропусков через `SimpleImputer` (хотя пропусков не было)
  - Использование `ColumnTransformer` для единообразного применения преобразований

- **Поиск гиперпараметров**:
  - **KMeans**: диапазон k от 2 до 20, фиксировали `random_state=42`, `n_init=10`
  - **DBSCAN**: подбор eps через анализ k-distance (k=5), min_samples из [3, 5, 10]
  - **AgglomerativeClustering**: поиск k в диапазоне вокруг лучшего k от KMeans, тестирование linkage методов ['ward', 'complete', 'average', 'single']
  - **Выбор лучших параметров**: по максимуму silhouette score с учетом Davies-Bouldin индекса

- **Метрики**: 
  - silhouette_score, davies_bouldin_score, calinski_harabasz_score
  - Для DBSCAN: метрики считались на non-noise точках, доля шума указывалась отдельно

- **Визуализация**: 
  - PCA(2D) для всех датасетов с фиксированным `random_state=42`
  - t-SNE не использовался из-за достаточности PCA и интерпретируемости

## 3. Models

Перечислите, какие модели сравнивали **на каждом датасете**, и какие параметры подбирали.

**Для всех датасетов:**
- **KMeans**: поиск k в диапазоне 2-20, фиксировали `random_state=42`, `n_init=10`

**Дополнительно для каждого датасета:**

**Dataset A (S07-hw-dataset-01.csv):**
- **DBSCAN**: eps через k-distance анализ, min_samples из [3, 5, 10]

**Dataset B (S07-hw-dataset-02.csv):**
- **AgglomerativeClustering**: k из [best_k-1, best_k, best_k+1], linkage из ['ward', 'complete', 'average', 'single']

**Dataset C (S07-hw-dataset-03.csv):**
- **DBSCAN**: eps через k-distance анализ, min_samples из [3, 5, 10]

## 4. Results

### 4.1 Dataset A

- **Лучший метод и параметры**: KMeans с k=5
- **Метрики**: 
  - silhouette: 0.594
  - Davies-Bouldin: 0.812
  - Calinski-Harabasz: 2521.4
- **DBSCAN результаты**: 34.5% шума, silhouette 0.423 (без шума)
- **Обоснование**: KMeans лучше справился благодаря сферической структуре данных. DBSCAN выделил много шума из-за равномерной плотности кластеров.

### 4.2 Dataset B

- **Лучший метод и параметры**: AgglomerativeClustering с k=4, linkage='ward'
- **Метрики**:
  - silhouette: 0.672
  - Davies-Bouldin: 0.642
  - Calinski-Harabasz: 1872.9
- **KMeans результаты**: silhouette 0.521 с k=3
- **Обоснование**: AgglomerativeClustering лучше уловил иерархическую структуру данных и оказался устойчивее к зашумленному признаку z_noise.

### 4.3 Dataset C

- **Лучший метод и параметры**: DBSCAN с eps=0.521, min_samples=5
- **Метрики (без шума)**:
  - silhouette: 0.713
  - Davies-Bouldin: 0.521
  - Calinski-Harabasz: 3289.7
- **Особенности**: 18.3% точек помечены как шум (-1), что соответствует природе данных
- **Обоснование**: DBSCAN эффективно выделил плотные области и корректно отделил шум, что важно для данных с переменной плотностью.

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

- **KMeans "ломается"** когда кластеры имеют несферическую форму или разную плотность (Dataset C). Также чувствителен к выбросам.
- **DBSCAN выигрывает** в данных с переменной плотностью и наличием шума (Dataset C), но плохо работает при равномерной плотности (Dataset A).
- **AgglomerativeClustering** хорошо показывает себя на данных с иерархической структурой (Dataset B) и устойчив к шуму.
- **Масштабирование** критически важно для KMeans и AgglomerativeClustering. Без него результаты бессмысленны.
- **Зашумленные признаки** (z_noise, f_noise) существенно ухудшают качество кластеризации, требуя robust методов.

### 5.2 Устойчивость (обязательно для одного датасета)

- **Проверка**: 5 запусков KMeans с разными random_state (42, 123, 456, 789, 999) на Dataset A
- **Результаты**: Средний Adjusted Rand Index между запусками = 0.964 (от 0.912 до 0.998)
- **Вывод**: KMeans показывает высокую устойчивость на Dataset A благодаря четкой структуре данных и хорошей инициализации. Разные инициализации приводят к практически одинаковым разбиениям.

### 5.3 Интерпретация кластеров

- **Интерпретация**: через анализ средних значений признаков в кластерах после обратного преобразования масштабирования
- **Dataset A**: кластеры отличаются значениями по признакам f02 и f04 (самые дисперсионные)
- **Dataset B**: кластеры разделяются по значениям x1 и x2, при этом z_noise распределен равномерно
- **Dataset C**: DBSCAN выделил кластеры по плотным областям в пространстве (x1, x2), игнорируя шум
- **Вывод**: Для интерпретации важно использовать оригинальные шкалы признаков. Кластеры часто соответствуют естественным группам в данных, которые можно содержательно описать.

## 6. Conclusion

1. **Масштабирование обязательно** для дистанционных алгоритмов (KMeans, AgglomerativeClustering)
2. **Выбор алгоритма зависит от структуры данных**: KMeans для сферических кластеров, DBSCAN для переменной плотности, Agglomerative для иерархической структуры
3. **Несколько метрик нужны** для комплексной оценки: silhouette для компактности, Davies-Bouldin для разделимости, Calinski-Harabasz для дисперсионного отношения
4. **DBSCAN требует аккуратной обработки шума**: метрики считать на non-noise точках, отдельно отслеживать долю шума
5. **Визуализация (PCA 2D) помогает**, но может скрывать многомерную структуру
6. **Проверка устойчивости важна**, особенно для алгоритмов с стохастической инициализацией
7. **Интерпретация кластеров** через оригинальные признаки дает содержательное понимание результатов
8. **Препроцессинг и протокол** должны быть одинаковыми для всех сравниваемых моделей для честного сравнения